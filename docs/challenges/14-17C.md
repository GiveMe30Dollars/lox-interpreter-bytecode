# 15-19C: Challenges from Chapters 14 to 17

## 14-1:

<blockquote>
Our encoding of line information is hilariously wasteful of memory. Given that a series of instructions often correspond to the same source line, a natural solution is something akin to run-length encoding of the line numbers.

Devise an encoding that compresses the line information for a series of instructions on the same line. Change writeChunk() to write this compressed form, and implement a getLine() function that, given the index of an instruction, determines the line where the instruction occurs.

Hint: It’s not necessary for getLine() to be particularly efficient. Since it is called only when a runtime error occurs, it is well off the critical path where performance matters.
</blockquote>

Run-length encoding (RLE) is done first with a dedicated `LineInfo` struct, which keeps the line and its run length. Afterwards, with reference to the original solution, a more efficient solution with information directly encoded within `chunk` is adopted, which keeps the line and the offset where the run begins. This more efficient encoding allowed for binary searching for `O(lg N)` complexity.

*Aside: While it is stated that `getLine()` doesn't need to be efficient, `O(N)` time complexity isn't great for getting line information lmao*

## 14-2:

<blockquote>
Because OP_CONSTANT uses only a single byte for its operand, a chunk may only contain up to 256 different constants. That’s small enough that people writing real-world code will hit that limit. We could use two or more bytes to store the operand, but that makes every constant instruction take up more space. Most chunks won’t need that many unique constants, so that wastes space and sacrifices some locality in the common case to support the rare case.

To balance those two competing aims, many instruction sets feature multiple instructions that perform the same operation but with operands of different sizes. Leave our existing one-byte OP_CONSTANT instruction alone, and define a second OP_CONSTANT_LONG instruction. It stores the operand as a 24-bit number, which should be plenty.

Implement this function:

void writeConstant(Chunk* chunk, Value value, int line) {
  // Implement me...
}


It adds value to chunk’s constant array and then writes an appropriate instruction to load the constant. Also add support to the disassembler for OP_CONSTANT_LONG instructions.

Defining two instructions seems to be the best of both worlds. What sacrifices, if any, does it force on us?
</blockquote>

See original solution: https://github.com/munificent/craftinginterpreters/blob/master/note/answers/chapter14_chunks/2.md

It should be noted that not only `OP_CONSTANT` requires an additional opcode; *anything* that uses a constants index requires a corresponding `LONG` variant. This includes the opcodes for interacting with global variables: `OP_DEFINE_GLOBAL`, `OP_GET_GLOBAL`, `OP_SET_GLOBAL`. This increases the number of opcodes to add to 4.

This has not been implemented as of writing. 

## 15-3:

<blockquote>
Our VM’s stack has a fixed size, and we don’t check if pushing a value overflows it. This means the wrong series of instructions could cause our interpreter to crash or go into undefined behavior. Avoid that by dynamically growing the stack as needed.

What are the costs and benefits of doing so?
</blockquote>

See original solution: https://github.com/munificent/craftinginterpreters/blob/master/note/answers/chapter15_virtual/3.md

This solution is complicated by the addition of `CallFrame` in Chapter 24, where the stack size is defined in relation to the maximum call frame stack (of `64`). The stack size is allocated to be `CALLFRAME_SIZE * UINT8_MAX`.

It should be noted that most language implementations allocate a fixed bytesize for the stack (https://docs.oracle.com/cd/E13150_01/jrockit_jvm/jrockit/jrdocs/refman/optionX.html#wp1024112), regardless of CallFrames.

## 16-1:

<blockquote>
Many newer languages support string interpolation. Inside a string literal, you have some sort of special delimiters—most commonly ${ at the beginning and } at the end. Between those delimiters, any expression can appear. When the string literal is executed, the inner expression is evaluated, converted to a string, and then merged with the surrounding string literal.

For example, if Lox supported string interpolation, then this . . . 

var drink = "Tea";
var steep = 4;
var cool = 2;
print "${drink} will be ready in ${steep + cool} minutes.";

 . . . would print:

Tea will be ready in 6 minutes.

What token types would you define to implement a scanner for string interpolation? What sequence of tokens would you emit for the above string literal?

What tokens would you emit for:

"Nested ${"interpolation?! Are you ${"mad?!"}"}"

Consider looking at other language implementations that support interpolation to see how they handle it.
</blockquote>

See original solution: https://github.com/munificent/craftinginterpreters/blob/master/note/answers/chapter16_scanning.md  
*Aside: imo this is a really cool way to do string interpolation (coming from a C/C++ `printf` scrub), formatters notwithstanding*

**Implementation Details:**

- **Scanning**: During string scanning, if matched `$`, emit current string as `TOKEN_INTERPOLATION`, consume subsequent `{`. Parse expression (default precedence `PREC_ASSIGN`), then consume `}`. Scan remaining string as `TOKEN_STRING` (unless another `$` is found, of course).  
  This requires the Scanner struct to know when it is in the middle of scanning a string (`int stringDepth` ?)

- **Compiling**: Treat `TOKEN_INTERPOLATION` as a prefix rule:
  - Push the corresponding `ObjString` onto the stack.
  - Evaluate the expression, then emit `OP_TO_STRING`, which pops the top of the stack and pushes an ObjString equivalent
  - Emit `OP_ADD` to concatenate both strings.
  - Check next token:
    - `TOKEN_INTERPOLATION`: recur.
    - `TOKEN_STRING`: push the string onto the stack. Then, emit `OP_ADD`. return.

- **VM**: Add VM and disassembler support for opcode `OP_TO_STRING` for all object types.

This seems really interesting! **Marked as TODO, along with C-style `printf` as a native function.**

## 17-2:

<blockquote>
The ParseRule row for TOKEN_MINUS has both prefix and infix function pointers. That’s because - is both a prefix operator (unary negation) and an infix one (subtraction).

In the full Lox language, what other tokens can be used in both prefix and infix positions? What about in C or in another language of your choice?
</blockquote>

See original solution: https://github.com/munificent/craftinginterpreters/blob/master/note/answers/chapter17_compiling.md

### `TOKEN_PLUS`

**Added: unary plus `+`.** `OP_UNARY_PLUS` is objectively a stupid waste of an opcode.  
May come back to this when a native `isType()` is supported.  
*Aside: Should types be first-class objects?*

### `TOKEN_LEFT_BRACKET`

I do plan on adding dynamic arrays and hash tables / hash sets into this implementation of Lox. Therefore, `[` would be both a prefix operator (array literals, destructuring statements) and infix operator (`Sequence` and `Mapping` access). Multiple inheritance should be added before that though...

### `TOKEN_COLON`

`:` is an odd one. The only contexts I plan to have them in is in the ternary conditional (`?:`), for key-value pairs of hash tables and for array slicing. In all three cases, `:` is used as a delimiter moreso than as an actual operator:

- Ternary conditional: `cond "?" yes ":" no`
  - `?` is detected as an infix operator for the ternary conditional.
  - `:` is consumed as part of the syntax for the ternary conditional.

- Hashtable literal: `"{"  (key ":" value ",")*  (key ":" value)?  "}"`  
  Related: hashset literal: `"{"  (key ",")*  key  "}"`
  - `{` is detected as a prefix operator for hashset / hashtable initialization. (block quotes are statements and are parsed before the Pratt Parser, thus this won't overlap)
  - If the body is empty, assume it is an empty dictionary.
  - Else, evaluate the first expression, then check for `:`. If it is there, confirmed to be a hashtable; parse the second expression (`value`).
  - Consume either `,` for more key-value pairs, or `}` for end of initialization.
  - If there exists no `:`, confirmed to be a hashset; consume either `,` for more expressions, or `}` for end of initialization.
  - *Note:* This follows Python's parsing of Dictionaries and Sets:
    ![Screenshot 2025-06-09 194707](https://github.com/user-attachments/assets/bce1837d-b92a-443c-9b9b-d60fc14e0a15)

- Array (Sequence) Slicing: `array "[" start? ":" stop? (":" step?)? "]"`  
  Related: single access of array or hashtable: `array "[" idx "]"`
  - `[` is detected as an infix operator.
  - Either: expression followed by `]`: single access.
  - Or: `:` or expression followed by `:`: slicing.

Refer to Python's usage of colons: https://www.askpython.com/python/examples/colon-in-python  
Refer also to Python's built-in types: https://docs.python.org/3/library/stdtypes.html#iterator-types

## 17-3:

<blockquote>
You might be wondering about complex “mixfix” expressions that have more than two operands separated by tokens. C’s conditional or “ternary” operator, ?:, is a widely known one.

Add support for that operator to the compiler. You don’t have to generate any bytecode, just show how you would hook it up to the parser and handle the operands.
</blockquote>

See original solution: https://github.com/munificent/craftinginterpreters/blob/master/note/answers/chapter17_compiling.md

**Added.** As described above in 17-2. The bytecode emitted is similar to that of an if-then-else control flow.  
*Note:* some implementation differences. Refer to 01E.

# 25-26C: Challenges from Chapters 25 to 26

## 25-1:

<blockquote>
Wrapping every ObjFunction in an ObjClosure introduces a level of indirection that has a performance cost. That cost isn’t necessary for functions that do not close over any variables, but it does let the runtime treat all calls uniformly.

Change clox to only wrap functions in ObjClosures that need upvalues. How does the code complexity and performance compare to always wrapping functions? Take care to benchmark programs that do and do not use closures. How should you weight the importance of each benchmark? If one gets slower and one faster, how do you decide what trade-off to make to choose an implementation strategy?
</blockquote>

See original solution: https://github.com/munificent/craftinginterpreters/blob/master/note/answers/chapter25_closures/1.md

I'm unsure as to whether I want to implement this.

I come from C/Java-land where closures are Not A Thing. This probably would bias me into preferring the implementation that is more performant for non-closure functions. It's hard to see which approach would enable that, though, since function and closure objects are usually not spam-created at runtime ([pathological, huh?](https://github.com/munificent/craftinginterpreters/blob/master/note/answers/chapter25_closures/1.md)) and are instead reused across the codebase.

Generally:
- Function calls: mild performance hit.
- Non-closure creation: major performance boost.
- Closure creation: unaffected.

**Update: added optional closures.**

Of course, the "pathological" testcase won out when it came to optional closures, but I was really interested in whether function calls would take a hit. I used the fibonacci testcase with `fib(40)`.

The results? Basically the same.

I'm pretty sure that I got an outlier value for one of these from running Firefox in the background, but for a best out of three, mandatory closures scored around 88.5 - *100.2* seconds, whereas optional closures scored 88.5 - 92.2 seconds.

That's... nothing. The conditional in `getFrameFunction` didn't impede performance for *shit*.

Keep in mind that while I am *not* going to model out the accurate number of function calls in `fib(40)`, if we naively assume each call to branch twice to `fib(n-1)`, that is an upper bound of *`2^40 - 1` function calls.* Divide that number by 2 if we assume that the shorter left branch causes the function space to be cut in half. It doesn't really matter.

If *this* didn't cause function calls to take a performance hit, nothing will.

# 25-2:

<blockquote>
Read the design note below. I’ll wait. Now, how do you think Lox should behave? Change the implementation to create a new variable for each loop iteration.
</blockquote>

**Added.** Implementation details around break and continue are described in Document [04I](../internal/04I_LoopVariableClosure.md).

# 26-1

<blockquote>
The Obj header struct at the top of each object now has three fields: type, isMarked, and next. How much memory do those take up (on your machine)? Can you come up with something more compact? Is there a runtime cost to doing so?
</blockquote>

See original solution: https://github.com/munificent/craftinginterpreters/blob/master/note/answers/chapter26_garbage/1.md

As suggested by Nystrom, I think that the lowest byte should be for the type, and the `next` pointer is to be bitshifted left. The garbage collector is not going to be fast anyways.

**Update: added. Can be toggled using flag `OBJ_HEADER_COMPRESSION`.**

Bit representation differed from original solution:

```
// Bit representation:
// .......M NNNNNNNN NNNNNNNN NNNNNNNN NNNNNNNN NNNNNNNN NNNNNNNN EEEEEEEE
// M: isMarked
// N: next pointer
// E: ObjType enum
```

Performance is nearly identical across both when benchmarked with [`closuretest.lox`](../../tests/closuretest.lox).

# 26-2

<blockquote>
When the sweep phase traverses a live object, it clears the isMarked field to prepare it for the next collection cycle. Can you come up with a more efficient approach?
</blockquote>

See original solution: https://github.com/munificent/craftinginterpreters/blob/master/note/answers/chapter26_garbage/2.md

I don't think this makes much sense to do *unless* done in conjunction with 26-1, where bitshifting shenanigans have a more obvious runtime cost than setting a `bool` in a C struct, which is practically free.

We're walking a linked list anyways. I don't think optimizing away the constants makes much of a difference in an `O(N)` mark-and-sweep.

# 26-3

<blockquote>
Mark-sweep is only one of a variety of garbage collection algorithms out there. Explore those by replacing or augmenting the current collector with another one. Good candidates to consider are reference counting, Cheney’s algorithm, or the Lisp 2 mark-compact algorithm.
</blockquote>

We are **not** doing reference counting. At least not by itself.

Consider:

```
class A {}
class B {}
{
    var a = A();
    var b = B();
    a.ref = b;
    b.ref = a;
}
```

`a` and `b` will never be freed. `a` has a reference count of 1, `b` has a reference count of 1. This is the *exact* problem my AST-based implementation faced: `std::shared_ptr` uses reference counting to determine when to free an object.

Of course, we can use this in conjunction with mark-and-sweep to eliminate out-of-scope cyclic references. While reference counting comes with its own overhead for storing the number of references an object has, it would be somewhat better than more frequent mark-and-sweeps. This *could* be considered. 

If this were to be implemented, `OP_POP` and friends would need some additional handling for informing the GC to decrement the reference count of popped objects. This does increase runtime overhead. Beisdes that, it'll probably also require turning the singly-linked list into a doubly-linked list unless I fancy traversing `vm.objects` every time I want to delete something.

Cheney's algorithm and Lisp 2 mark-compact algorithm are both interesting, but are kinda infeasible to implement unless I go "hardcore mode" and also implement my own heap. ... I think I'll pass for now, thanks.

Real quick, though:

- Cheney's algorithm involves allocating two heap spaces, where only one is used at any time. When marking, also copy over the contents into the other heap space. Then, deallocate the entire old heap space. Marked objects survive; unmarked ones don't.
  - It is very conceptually simple, just that there's like an entire heap space worth of *nothing*.

- The Lisp 2 mark-compact algorithm can be thought of as a combination of mark-and-sweep and Cheney's algorithm. 
  - A mark-compact algorithm exists to counteract memory fragmentation by compacting the objects' data together after each GC. 
  - The Lisp 2 algorithm is created to counteract one of the weaknesses of a typical mark-compact algorithm: that the break table (which stores where the old and relocated positions of the data are) needs to be sorted in `O(N lg N)` time as the algorithm progresses and the table is "rolled" into unused memory.
  - An extra "forwarding" pointer is kept in all objects.
  - After standard marking, the algorithm proceeds in the following three passes:
    - Compute the forwarding location for live objects.
      - Keep track of a free and live pointer and initialize both to the start of heap.
      - If the live pointer points to a live object, update that object's forwarding pointer to the current free pointer and increment the free pointer according to the object's size.
      - Move the live pointer to the next object
      - End when the live pointer reaches the end of heap.
    - Update all pointers
      - For each live object, update its pointers according to the forwarding pointers of the objects they point to.
    - Move objects
      - For each live object, move its data to its forwarding location.

    *Source: [Wikipedia](https://en.wikipedia.org/wiki/Mark%E2%80%93compact_algorithm#LISP_2_algorithm)*.